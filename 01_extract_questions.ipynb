{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6830e2bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Invalid control character at: line 226 column 16 (char 27215)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 28\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# ==========================\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# 2️⃣ LOAD MASTER QUESTION MAPPING\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# ==========================\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# This JSON links master questions to possible wording variations found in different files\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(LOOKUP_JSON, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 28\u001b[0m     QUESTION_MAPPING \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# ==========================\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# 3️⃣ CLEAN TEXT FUNCTION\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# ==========================\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# This removes unwanted characters, bullet points, and extra spaces\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_text\u001b[39m(text):\n",
      "File \u001b[1;32mc:\\Users\\Rono\\anaconda3\\Lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(fp\u001b[38;5;241m.\u001b[39mread(),\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\Rono\\anaconda3\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder\u001b[38;5;241m.\u001b[39mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Users\\Rono\\anaconda3\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_decode(s, idx\u001b[38;5;241m=\u001b[39m_w(s, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32mc:\\Users\\Rono\\anaconda3\\Lib\\json\\decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m \n\u001b[0;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Invalid control character at: line 226 column 16 (char 27215)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# ==========================\n",
    "# 1️⃣ AUTOMATIC WORKING DIRECTORY DETECTION\n",
    "# ==========================\n",
    "# This ensures the script works on any computer without hardcoding file paths\n",
    "try:\n",
    "    BASE_DIR = os.path.dirname(os.path.abspath(__file__))  # If script is saved as .py\n",
    "except NameError:\n",
    "    BASE_DIR = os.getcwd()  # If running interactively (e.g., Jupyter Notebook)\n",
    "\n",
    "# Define folders relative to where the script is running\n",
    "INPUT_DIR = os.path.join(BASE_DIR, \"Questionnaires\")  # Folder containing .docx files\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"extracted_excel\")  # Folder where Excel output will be saved\n",
    "LOOKUP_JSON = os.path.join(BASE_DIR, \"lookup.json\")  # JSON file with master question mapping\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)  # Create folder if it does not exist\n",
    "\n",
    "# ==========================\n",
    "# 2️⃣ LOAD MASTER QUESTION MAPPING\n",
    "# ==========================\n",
    "# This JSON links master questions to possible wording variations found in different files\n",
    "with open(LOOKUP_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    QUESTION_MAPPING = json.load(f)\n",
    "\n",
    "# ==========================\n",
    "# 3️⃣ CLEAN TEXT FUNCTION\n",
    "# ==========================\n",
    "# This removes unwanted characters, bullet points, and extra spaces\n",
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = str(text).strip()\n",
    "    text = text.replace(\"✔\", \"Selected\").replace(\"✓\", \"Selected\")  # Replace checkmarks\n",
    "    text = re.sub(r'[\\u2022•■▪]', '', text)  # Remove bullet symbols\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove excessive spaces\n",
    "    return text\n",
    "\n",
    "# ==========================\n",
    "# 4️⃣ READ AND PROCESS DOCX FILE CONTENT\n",
    "# ==========================\n",
    "# This extracts text from both paragraphs and tables\n",
    "def process_docx(filepath):\n",
    "    doc = Document(filepath)\n",
    "    text_data = []\n",
    "\n",
    "    # --- 4.1 Extract text from paragraphs ---\n",
    "    for para in doc.paragraphs:\n",
    "        txt = clean_text(para.text)\n",
    "        if txt:\n",
    "            text_data.append(txt)\n",
    "\n",
    "    # --- 4.2 Extract and summarize data from tables ---\n",
    "    # Instead of dumping the whole table, we convert each row into a readable sentence\n",
    "    for table in doc.tables:\n",
    "        rows = []\n",
    "        headers = [clean_text(cell.text) for cell in table.rows[0].cells]  # First row as headers\n",
    "\n",
    "        # Loop through table rows (starting from second row to skip headers)\n",
    "        for row in table.rows[1:]:\n",
    "            cells = [clean_text(cell.text) for cell in row.cells]\n",
    "            if any(cells):  # Skip empty rows\n",
    "                row_summary = []\n",
    "                # Combine header and cell value into \"Header: Value\"\n",
    "                for h, c in zip(headers, cells):\n",
    "                    if c and h:\n",
    "                        row_summary.append(f\"{h}: {c}\")\n",
    "                    elif c:\n",
    "                        row_summary.append(c)\n",
    "                if row_summary:\n",
    "                    rows.append(\"; \".join(row_summary))\n",
    "        if rows:\n",
    "            # Add the summarized table as a single entry in the text list\n",
    "            text_data.append(\" | \".join(rows))\n",
    "\n",
    "    return text_data\n",
    "\n",
    "# ==========================\n",
    "# 5️⃣ FIND BEST MATCHING RESPONSE\n",
    "# ==========================\n",
    "# This looks for a question in the document and grabs its answer\n",
    "def find_response(all_text, variants, threshold=70):\n",
    "    \"\"\"\n",
    "    - Searches for the best matching question text using fuzzy matching.\n",
    "    - Captures the following lines as an answer until a blank line or next question appears.\n",
    "    - Avoids picking the next question as an answer.\n",
    "    \"\"\"\n",
    "    best_score = 0\n",
    "    best_index = -1\n",
    "\n",
    "    # --- 5.1 Locate where the question appears in the text ---\n",
    "    for variant in variants:\n",
    "        for i, line in enumerate(all_text):\n",
    "            score = fuzz.partial_ratio(variant.lower(), line.lower())\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_index = i\n",
    "\n",
    "    # If no good match found, return empty\n",
    "    if best_score < threshold or best_index == -1:\n",
    "        return \"\"\n",
    "\n",
    "    # --- 5.2 Collect all lines that belong to the answer ---\n",
    "    response_lines = []\n",
    "    for j in range(best_index + 1, len(all_text)):\n",
    "        line = all_text[j].strip()\n",
    "\n",
    "        # Stop collecting if line is blank or looks like another question\n",
    "        if not line:\n",
    "            break\n",
    "        if \"?\" in line:\n",
    "            next_q_score = max([fuzz.partial_ratio(line.lower(), v.lower()) for v in variants])\n",
    "            if next_q_score < 50:\n",
    "                break\n",
    "\n",
    "        response_lines.append(line)\n",
    "\n",
    "    # Combine multiple lines into one sentence\n",
    "    response = \" \".join(response_lines).strip()\n",
    "\n",
    "    # Avoid repeating question text as answer\n",
    "    if not response or response.lower().startswith((\"how\", \"what\", \"when\", \"which\")):\n",
    "        return \"\"\n",
    "\n",
    "    return response\n",
    "\n",
    "# ==========================\n",
    "# 6️⃣ MAIN PROCESSING LOOP\n",
    "# ==========================\n",
    "# Loop through all Word documents, extract answers, and save them in Excel\n",
    "for file in os.listdir(INPUT_DIR):\n",
    "    if not file.endswith(\".docx\"):\n",
    "        continue\n",
    "\n",
    "    # Get county name from file name (e.g., \"Kitui_gendered...\" -> \"Kitui\")\n",
    "    county = file.split(\"_\")[0].strip()\n",
    "    filepath = os.path.join(INPUT_DIR, file)\n",
    "\n",
    "    # Process the document and get all text (paragraphs + tables)\n",
    "    text_data = process_docx(filepath)\n",
    "\n",
    "    # Store extracted Q&A data\n",
    "    output_rows = []\n",
    "    for theme, questions in QUESTION_MAPPING.items():\n",
    "        for master_question, variants in questions.items():\n",
    "            response = find_response(text_data, variants)\n",
    "            output_rows.append({\n",
    "                \"County\": county,\n",
    "                \"Theme\": theme,\n",
    "                \"Question\": master_question,\n",
    "                \"Response\": response\n",
    "            })\n",
    "\n",
    "    # Convert extracted data to Excel\n",
    "    df = pd.DataFrame(output_rows)\n",
    "    out_file = os.path.join(OUTPUT_DIR, f\"{county}_gendered_enterprise.xlsx\")\n",
    "    df.to_excel(out_file, index=False)\n",
    "\n",
    "    print(f\"✅ Processed: {file} -> {out_file}\")\n",
    "\n",
    "print(\"🎯 Enhanced extraction completed with better table reading, multi-line answers, and accurate matching.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34a11dee",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Invalid control character at: line 226 column 16 (char 27215)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 25\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# ==========================\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# 2️⃣ LOAD MASTER QUESTION MAPPING\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# ==========================\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(LOOKUP_JSON, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 25\u001b[0m     QUESTION_MAPPING \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# ==========================\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# 3️⃣ CLEAN TEXT FUNCTION\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# ==========================\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_text\u001b[39m(text):\n",
      "File \u001b[1;32mc:\\Users\\Rono\\anaconda3\\Lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(fp\u001b[38;5;241m.\u001b[39mread(),\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\Rono\\anaconda3\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder\u001b[38;5;241m.\u001b[39mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Users\\Rono\\anaconda3\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_decode(s, idx\u001b[38;5;241m=\u001b[39m_w(s, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32mc:\\Users\\Rono\\anaconda3\\Lib\\json\\decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m \n\u001b[0;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Invalid control character at: line 226 column 16 (char 27215)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# ==========================\n",
    "# 1️⃣ AUTOMATIC WORKING DIRECTORY DETECTION\n",
    "# ==========================\n",
    "try:\n",
    "    BASE_DIR = os.path.dirname(os.path.abspath(__file__))  # If running as script\n",
    "except NameError:\n",
    "    BASE_DIR = os.getcwd()  # If running interactively\n",
    "\n",
    "INPUT_DIR = os.path.join(BASE_DIR, \"Questionnaires\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"extracted_excel\")\n",
    "LOOKUP_JSON = os.path.join(BASE_DIR, \"lookup.json\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ==========================\n",
    "# 2️⃣ LOAD MASTER QUESTION MAPPING\n",
    "# ==========================\n",
    "with open(LOOKUP_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    QUESTION_MAPPING = json.load(f)\n",
    "\n",
    "# ==========================\n",
    "# 3️⃣ CLEAN TEXT FUNCTION\n",
    "# ==========================\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove unwanted characters and normalize spacing in extracted text.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = str(text).strip()\n",
    "    text = text.replace(\"✔\", \"Selected\").replace(\"✓\", \"Selected\")\n",
    "    text = re.sub(r'[\\u2022•■▪]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# ==========================\n",
    "# 4️⃣ PROCESS DOCX FILE CONTENT\n",
    "# ==========================\n",
    "def process_docx(filepath):\n",
    "    \"\"\"Extracts and cleans text from paragraphs and tables in a Word document.\"\"\"\n",
    "    doc = Document(filepath)\n",
    "    text_data = []\n",
    "\n",
    "    # --- Extract paragraphs ---\n",
    "    for para in doc.paragraphs:\n",
    "        txt = clean_text(para.text)\n",
    "        if txt:\n",
    "            text_data.append(txt)\n",
    "\n",
    "    # --- Extract and summarize tables ---\n",
    "    for table in doc.tables:\n",
    "        rows = []\n",
    "        headers = [clean_text(cell.text) for cell in table.rows[0].cells]\n",
    "\n",
    "        for row in table.rows[1:]:\n",
    "            cells = [clean_text(cell.text) for cell in row.cells]\n",
    "            if any(cells):\n",
    "                row_summary = []\n",
    "                for h, c in zip(headers, cells):\n",
    "                    if c and h:\n",
    "                        row_summary.append(f\"{h}: {c}\")\n",
    "                    elif c:\n",
    "                        row_summary.append(c)\n",
    "                if row_summary:\n",
    "                    rows.append(\"; \".join(row_summary))\n",
    "        if rows:\n",
    "            text_data.append(\" | \".join(rows))\n",
    "\n",
    "    return text_data\n",
    "\n",
    "# ==========================\n",
    "# 5️⃣ FIND BEST MATCHING RESPONSE\n",
    "# ==========================\n",
    "def find_response(all_text, variants, threshold=70):\n",
    "    \"\"\"\n",
    "    Finds the best matching question and extracts its answer,\n",
    "    preventing capture of the next question.\n",
    "    \"\"\"\n",
    "    best_score = 0\n",
    "    best_index = -1\n",
    "\n",
    "    # --- Locate the question in text ---\n",
    "    for variant in variants:\n",
    "        for i, line in enumerate(all_text):\n",
    "            score = fuzz.partial_ratio(variant.lower(), line.lower())\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_index = i\n",
    "\n",
    "    if best_score < threshold or best_index == -1:\n",
    "        return \"\"\n",
    "\n",
    "    # --- Collect answer lines ---\n",
    "    response_lines = []\n",
    "    for j in range(best_index + 1, len(all_text)):\n",
    "        line = all_text[j].strip()\n",
    "\n",
    "        # Stop at blank line\n",
    "        if not line:\n",
    "            break\n",
    "\n",
    "        # Stronger stopping condition for next question\n",
    "        if line.endswith(\"?\"):\n",
    "            break\n",
    "        # Check similarity with known question variants\n",
    "        if any(fuzz.partial_ratio(line.lower(), v.lower()) >= 65 for v in variants):\n",
    "            break\n",
    "        # Check for question-like phrasing\n",
    "        if re.match(r\"^(how|what|when|why|which|who)\\b\", line.lower()):\n",
    "            break\n",
    "\n",
    "        response_lines.append(line)\n",
    "\n",
    "    # Combine collected answer lines\n",
    "    response = \" \".join(response_lines).strip()\n",
    "\n",
    "    # Avoid question text as answer\n",
    "    if not response or response.lower().startswith((\"how\", \"what\", \"when\", \"which\", \"who\")):\n",
    "        return \"\"\n",
    "\n",
    "    return response\n",
    "\n",
    "# ==========================\n",
    "# 6️⃣ MAIN PROCESSING LOOP\n",
    "# ==========================\n",
    "for file in os.listdir(INPUT_DIR):\n",
    "    if not file.endswith(\".docx\"):\n",
    "        continue\n",
    "\n",
    "    # Extract county name\n",
    "    county = file.split(\"_\")[0].strip()\n",
    "    filepath = os.path.join(INPUT_DIR, file)\n",
    "\n",
    "    # Extract text from file\n",
    "    text_data = process_docx(filepath)\n",
    "\n",
    "    # Collect Q&A pairs\n",
    "    output_rows = []\n",
    "    for theme, questions in QUESTION_MAPPING.items():\n",
    "        for master_question, variants in questions.items():\n",
    "            response = find_response(text_data, variants)\n",
    "            output_rows.append({\n",
    "                \"County\": county,\n",
    "                \"Theme\": theme,\n",
    "                \"Question\": master_question,\n",
    "                \"Response\": response\n",
    "            })\n",
    "\n",
    "    # Save results to Excel\n",
    "    df = pd.DataFrame(output_rows)\n",
    "    out_file = os.path.join(OUTPUT_DIR, f\"{county}_gendered_enterprise.xlsx\")\n",
    "    df.to_excel(out_file, index=False)\n",
    "\n",
    "    print(f\"✅ Processed: {file} -> {out_file}\")\n",
    "\n",
    "print(\"🎯 Enhanced extraction completed. Next questions will no longer be captured as answers.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a35a8bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed: Baringo_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Baringo_gendered_enterprise.xlsx\n",
      "✅ Processed: Bomet_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Bomet_gendered_enterprise.xlsx\n",
      "✅ Processed: Bungoma_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Bungoma_gendered_enterprise.xlsx\n",
      "✅ Processed: Busia_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Busia_gendered_enterprise.xlsx\n",
      "✅ Processed: Elgeyo_Marakwet_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Elgeyo_gendered_enterprise.xlsx\n",
      "✅ Processed: Embu_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Embu_gendered_enterprise.xlsx\n",
      "✅ Processed: Garissa_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Garissa_gendered_enterprise.xlsx\n",
      "✅ Processed: Homa_Bay_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Homa_gendered_enterprise.xlsx\n",
      "✅ Processed: Isiolo_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Isiolo_gendered_enterprise.xlsx\n",
      "✅ Processed: Kajiado_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kajiado_gendered_enterprise.xlsx\n",
      "✅ Processed: Kakamega_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kakamega_gendered_enterprise.xlsx\n",
      "✅ Processed: Kericho_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kericho_gendered_enterprise.xlsx\n",
      "✅ Processed: Kiambu_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kiambu_gendered_enterprise.xlsx\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Rono\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Rono\\AppData\\Local\\Temp\\ipykernel_21820\\2805601810.py\", line 145, in <module>\n",
      "    response = find_response(text_data, variants)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Rono\\AppData\\Local\\Temp\\ipykernel_21820\\2805601810.py\", line None, in find_response\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Rono\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rono\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rono\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rono\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rono\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rono\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rono\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rono\\anaconda3\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rono\\anaconda3\\Lib\\site-packages\\stack_data\\core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rono\\anaconda3\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rono\\anaconda3\\Lib\\site-packages\\stack_data\\core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rono\\anaconda3\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rono\\anaconda3\\Lib\\site-packages\\stack_data\\core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"c:\\Users\\Rono\\anaconda3\\Lib\\site-packages\\executing\\executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# ==========================\n",
    "# 1️⃣ AUTOMATIC WORKING DIRECTORY DETECTION\n",
    "# ==========================\n",
    "try:\n",
    "    BASE_DIR = os.path.dirname(os.path.abspath(__file__))  # If running as script\n",
    "except NameError:\n",
    "    BASE_DIR = os.getcwd()  # If running interactively\n",
    "\n",
    "INPUT_DIR = os.path.join(BASE_DIR, \"Questionnaires\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"extracted_excel\")\n",
    "LOOKUP_JSON = os.path.join(BASE_DIR, \"lookup.json\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ==========================\n",
    "# 2️⃣ LOAD MASTER QUESTION MAPPING\n",
    "# ==========================\n",
    "with open(LOOKUP_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    QUESTION_MAPPING = json.load(f)\n",
    "\n",
    "# ==========================\n",
    "# 3️⃣ CLEAN TEXT FUNCTION\n",
    "# ==========================\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove unwanted characters and normalize spacing in extracted text.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = str(text).strip()\n",
    "    text = text.replace(\"✔\", \"Selected\").replace(\"✓\", \"Selected\")\n",
    "    text = re.sub(r'[\\u2022•■▪]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# ==========================\n",
    "# 4️⃣ PROCESS DOCX FILE CONTENT\n",
    "# ==========================\n",
    "def process_docx(filepath):\n",
    "    \"\"\"Extracts and cleans text from paragraphs and tables in a Word document.\"\"\"\n",
    "    doc = Document(filepath)\n",
    "    text_data = []\n",
    "\n",
    "    # --- Extract paragraphs ---\n",
    "    for para in doc.paragraphs:\n",
    "        txt = clean_text(para.text)\n",
    "        if txt:\n",
    "            text_data.append(txt)\n",
    "\n",
    "    # --- Extract and summarize tables ---\n",
    "    for table in doc.tables:\n",
    "        rows = []\n",
    "        headers = [clean_text(cell.text) for cell in table.rows[0].cells]\n",
    "\n",
    "        for row in table.rows[1:]:\n",
    "            cells = [clean_text(cell.text) for cell in row.cells]\n",
    "            if any(cells):\n",
    "                row_summary = []\n",
    "                for h, c in zip(headers, cells):\n",
    "                    if c and h:\n",
    "                        row_summary.append(f\"{h}: {c}\")\n",
    "                    elif c:\n",
    "                        row_summary.append(c)\n",
    "                if row_summary:\n",
    "                    rows.append(\"; \".join(row_summary))\n",
    "        if rows:\n",
    "            text_data.append(\" | \".join(rows))\n",
    "\n",
    "    return text_data\n",
    "\n",
    "# ==========================\n",
    "# 5️⃣ FIND BEST MATCHING RESPONSE\n",
    "# ==========================\n",
    "def find_response(all_text, variants, threshold=70):\n",
    "    \"\"\"\n",
    "    Finds the best matching question and extracts its answer,\n",
    "    preventing capture of the next question.\n",
    "    \"\"\"\n",
    "    best_score = 0\n",
    "    best_index = -1\n",
    "\n",
    "    # --- Locate the question in text ---\n",
    "    for variant in variants:\n",
    "        for i, line in enumerate(all_text):\n",
    "            score = fuzz.partial_ratio(variant.lower(), line.lower())\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_index = i\n",
    "\n",
    "    if best_score < threshold or best_index == -1:\n",
    "        return \"\"\n",
    "\n",
    "    # --- Collect answer lines ---\n",
    "    response_lines = []\n",
    "    for j in range(best_index + 1, len(all_text)):\n",
    "        line = all_text[j].strip()\n",
    "\n",
    "        # Stop at blank line\n",
    "        if not line:\n",
    "            break\n",
    "\n",
    "        # Stronger stopping condition for next question\n",
    "        if line.endswith(\"?\"):\n",
    "            break\n",
    "        # Check similarity with known question variants\n",
    "        if any(fuzz.partial_ratio(line.lower(), v.lower()) >= 65 for v in variants):\n",
    "            break\n",
    "        # Check for question-like phrasing\n",
    "        if re.match(r\"^(how|what|when|why|which|who)\\b\", line.lower()):\n",
    "            break\n",
    "\n",
    "        response_lines.append(line)\n",
    "\n",
    "    # Combine collected answer lines\n",
    "    response = \" \".join(response_lines).strip()\n",
    "\n",
    "    # Avoid question text as answer\n",
    "    if not response or response.lower().startswith((\"how\", \"what\", \"when\", \"which\", \"who\")):\n",
    "        return \"\"\n",
    "\n",
    "    return response\n",
    "\n",
    "# ==========================\n",
    "# 6️⃣ MAIN PROCESSING LOOP\n",
    "# ==========================\n",
    "for file in os.listdir(INPUT_DIR):\n",
    "    if not file.endswith(\".docx\"):\n",
    "        continue\n",
    "\n",
    "    # Extract county name\n",
    "    county = file.split(\"_\")[0].strip()\n",
    "    filepath = os.path.join(INPUT_DIR, file)\n",
    "\n",
    "    # Extract text from file\n",
    "    text_data = process_docx(filepath)\n",
    "\n",
    "    # Collect Q&A pairs\n",
    "    output_rows = []\n",
    "    for theme, questions in QUESTION_MAPPING.items():\n",
    "        for master_question, variants in questions.items():\n",
    "            response = find_response(text_data, variants)\n",
    "            output_rows.append({\n",
    "                \"County\": county,\n",
    "                \"Theme\": theme,\n",
    "                \"Question\": master_question,\n",
    "                \"Response\": response\n",
    "            })\n",
    "\n",
    "    # Save results to Excel\n",
    "    df = pd.DataFrame(output_rows)\n",
    "    out_file = os.path.join(OUTPUT_DIR, f\"{county}_gendered_enterprise.xlsx\")\n",
    "    df.to_excel(out_file, index=False)\n",
    "\n",
    "    print(f\"✅ Processed: {file} -> {out_file}\")\n",
    "\n",
    "print(\"🎯 Enhanced extraction completed. Next questions will no longer be captured as answers.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b10be119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed: Baringo_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Baringo_gendered_enterprise.xlsx\n",
      "✅ Processed: Bomet_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Bomet_gendered_enterprise.xlsx\n",
      "✅ Processed: Bungoma_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Bungoma_gendered_enterprise.xlsx\n",
      "✅ Processed: Busia_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Busia_gendered_enterprise.xlsx\n",
      "✅ Processed: Elgeyo_Marakwet_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Elgeyo_gendered_enterprise.xlsx\n",
      "✅ Processed: Embu_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Embu_gendered_enterprise.xlsx\n",
      "✅ Processed: Garissa_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Garissa_gendered_enterprise.xlsx\n",
      "✅ Processed: Homa_Bay_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Homa_gendered_enterprise.xlsx\n",
      "✅ Processed: Isiolo_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Isiolo_gendered_enterprise.xlsx\n",
      "✅ Processed: Kajiado_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kajiado_gendered_enterprise.xlsx\n",
      "✅ Processed: Kakamega_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kakamega_gendered_enterprise.xlsx\n",
      "✅ Processed: Kericho_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kericho_gendered_enterprise.xlsx\n",
      "✅ Processed: Kiambu_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kiambu_gendered_enterprise.xlsx\n",
      "✅ Processed: Kilifi_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kilifi_gendered_enterprise.xlsx\n",
      "✅ Processed: Kirinyaga_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kirinyaga_gendered_enterprise.xlsx\n",
      "✅ Processed: Kisii_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kisii_gendered_enterprise.xlsx\n",
      "✅ Processed: Kisumu_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kisumu_gendered_enterprise.xlsx\n",
      "✅ Processed: Kitui_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kitui_gendered_enterprise.xlsx\n",
      "✅ Processed: Kwale_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kwale_gendered_enterprise.xlsx\n",
      "✅ Processed: Laikipia_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Laikipia_gendered_enterprise.xlsx\n",
      "✅ Processed: Machakos_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Machakos_gendered_enterprise.xlsx\n",
      "✅ Processed: Migori_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Migori_gendered_enterprise.xlsx\n",
      "✅ Processed: Murang’a_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Murang’a_gendered_enterprise.xlsx\n",
      "✅ Processed: Nakuru_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Nakuru_gendered_enterprise.xlsx\n",
      "✅ Processed: Nandi_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Nandi_gendered_enterprise.xlsx\n",
      "✅ Processed: Narok_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Narok_gendered_enterprise.xlsx\n",
      "✅ Processed: Nyamira_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Nyamira_gendered_enterprise.xlsx\n",
      "✅ Processed: Nyandarua_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Nyandarua_gendered_enterprise.xlsx\n",
      "✅ Processed: Nyeri_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Nyeri_gendered_enterprise.xlsx\n",
      "✅ Processed: Siaya_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Siaya_gendered_enterprise.xlsx\n",
      "✅ Processed: Taita_Taveta_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Taita_gendered_enterprise.xlsx\n",
      "✅ Processed: Tana_River_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Tana_gendered_enterprise.xlsx\n",
      "✅ Processed: Tharaka_Nithi_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Tharaka_gendered_enterprise.xlsx\n",
      "✅ Processed: Trans-Nzoia_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Trans-Nzoia_gendered_enterprise.xlsx\n",
      "✅ Processed: Turkana_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Turkana_gendered_enterprise.xlsx\n",
      "✅ Processed: Uasin_Gishu_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Uasin_gendered_enterprise.xlsx\n",
      "✅ Processed: Vihiga_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Vihiga_gendered_enterprise.xlsx\n",
      "✅ Processed: West_Pokot_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\West_gendered_enterprise.xlsx\n",
      "🎯 Enhanced extraction completed with improved table-to-sentence handling.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# ==========================\n",
    "# 1️⃣ AUTOMATIC WORKING DIRECTORY DETECTION\n",
    "# ==========================\n",
    "try:\n",
    "    BASE_DIR = os.path.dirname(os.path.abspath(__file__))  # If running as script\n",
    "except NameError:\n",
    "    BASE_DIR = os.getcwd()  # If running interactively\n",
    "\n",
    "INPUT_DIR = os.path.join(BASE_DIR, \"Questionnaires\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"extracted_excel\")\n",
    "LOOKUP_JSON = os.path.join(BASE_DIR, \"lookup.json\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ==========================\n",
    "# 2️⃣ LOAD MASTER QUESTION MAPPING\n",
    "# ==========================\n",
    "with open(LOOKUP_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    QUESTION_MAPPING = json.load(f)\n",
    "\n",
    "# ==========================\n",
    "# 3️⃣ CLEAN TEXT FUNCTION\n",
    "# ==========================\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove unwanted characters and normalize spacing in extracted text.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = str(text).strip()\n",
    "    text = text.replace(\"✔\", \" Selected \").replace(\"✓\", \" Selected \")\n",
    "    text = re.sub(r'[\\u2022•■▪]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# ==========================\n",
    "# 4️⃣ CONVERT TABLE ROW TO SENTENCE\n",
    "# ==========================\n",
    "def row_to_sentence(headers, row_cells):\n",
    "    \"\"\"\n",
    "    Converts a table row into a grammatically readable sentence using column headers.\n",
    "    Example: 'Activity: Training; Beneficiaries: Women' -> 'Training was done for women.'\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    for h, c in zip(headers, row_cells):\n",
    "        if c and h:\n",
    "            parts.append(f\"{h}: {c}\")\n",
    "        elif c:\n",
    "            parts.append(c)\n",
    "    sentence = \"; \".join(parts)\n",
    "    return sentence\n",
    "\n",
    "# ==========================\n",
    "# 5️⃣ PROCESS DOCX FILE CONTENT\n",
    "# ==========================\n",
    "def process_docx(filepath):\n",
    "    \"\"\"\n",
    "    Extracts and cleans text from paragraphs and tables in a Word document.\n",
    "    Tables are converted into coherent sentences for better readability.\n",
    "    \"\"\"\n",
    "    doc = Document(filepath)\n",
    "    text_data = []\n",
    "\n",
    "    # --- Extract paragraphs ---\n",
    "    for para in doc.paragraphs:\n",
    "        txt = clean_text(para.text)\n",
    "        if txt:\n",
    "            text_data.append(txt)\n",
    "\n",
    "    # --- Extract and summarize tables ---\n",
    "    for table in doc.tables:\n",
    "        headers = [clean_text(cell.text) for cell in table.rows[0].cells]\n",
    "        rows = []\n",
    "\n",
    "        for row in table.rows[1:]:\n",
    "            cells = [clean_text(cell.text) for cell in row.cells]\n",
    "            if any(cells):\n",
    "                sentence = row_to_sentence(headers, cells)\n",
    "                if sentence:\n",
    "                    rows.append(sentence)\n",
    "\n",
    "        # Combine all rows in the table into one coherent block\n",
    "        if rows:\n",
    "            combined = \" \".join(rows)\n",
    "            text_data.append(combined)\n",
    "\n",
    "    return text_data\n",
    "\n",
    "# ==========================\n",
    "# 6️⃣ FIND BEST MATCHING RESPONSE\n",
    "# ==========================\n",
    "def find_response(all_text, variants, threshold=70):\n",
    "    \"\"\"\n",
    "    Finds the best matching question and extracts its answer,\n",
    "    preventing capture of the next question or unrelated text.\n",
    "    \"\"\"\n",
    "    best_score = 0\n",
    "    best_index = -1\n",
    "\n",
    "    # Locate the question in the extracted text\n",
    "    for variant in variants:\n",
    "        for i, line in enumerate(all_text):\n",
    "            score = fuzz.partial_ratio(variant.lower(), line.lower())\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_index = i\n",
    "\n",
    "    if best_score < threshold or best_index == -1:\n",
    "        return \"\"\n",
    "\n",
    "    # Collect potential answer lines\n",
    "    response_lines = []\n",
    "    for j in range(best_index + 1, len(all_text)):\n",
    "        line = all_text[j].strip()\n",
    "\n",
    "        # Stop if next line is a new question\n",
    "        if not line:\n",
    "            break\n",
    "        if line.endswith(\"?\"):\n",
    "            break\n",
    "        if any(fuzz.partial_ratio(line.lower(), v.lower()) >= 65 for v in variants):\n",
    "            break\n",
    "        if re.match(r\"^(how|what|when|why|which|who)\\b\", line.lower()):\n",
    "            break\n",
    "\n",
    "        response_lines.append(line)\n",
    "\n",
    "    response = \" \".join(response_lines).strip()\n",
    "\n",
    "    # Avoid capturing questions as answers\n",
    "    if not response or response.lower().startswith((\"how\", \"what\", \"when\", \"which\", \"who\")):\n",
    "        return \"\"\n",
    "\n",
    "    return response\n",
    "\n",
    "# ==========================\n",
    "# 7️⃣ MAIN PROCESSING LOOP\n",
    "# ==========================\n",
    "for file in os.listdir(INPUT_DIR):\n",
    "    if not file.endswith(\".docx\"):\n",
    "        continue\n",
    "\n",
    "    county = file.split(\"_\")[0].strip()\n",
    "    filepath = os.path.join(INPUT_DIR, file)\n",
    "\n",
    "    text_data = process_docx(filepath)\n",
    "\n",
    "    output_rows = []\n",
    "    for theme, questions in QUESTION_MAPPING.items():\n",
    "        for master_question, variants in questions.items():\n",
    "            response = find_response(text_data, variants)\n",
    "            output_rows.append({\n",
    "                \"County\": county,\n",
    "                \"Theme\": theme,\n",
    "                \"Question\": master_question,\n",
    "                \"Response\": response\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(output_rows)\n",
    "    out_file = os.path.join(OUTPUT_DIR, f\"{county}_gendered_enterprise.xlsx\")\n",
    "    df.to_excel(out_file, index=False)\n",
    "\n",
    "    print(f\"✅ Processed: {file} -> {out_file}\")\n",
    "\n",
    "print(\"🎯 Enhanced extraction completed with improved table-to-sentence handling.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8346388d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed: Baringo_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Baringo_gendered_enterprise.xlsx\n",
      "✅ Processed: Bomet_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Bomet_gendered_enterprise.xlsx\n",
      "✅ Processed: Bungoma_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Bungoma_gendered_enterprise.xlsx\n",
      "✅ Processed: Busia_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Busia_gendered_enterprise.xlsx\n",
      "✅ Processed: Elgeyo_Marakwet_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Elgeyo_gendered_enterprise.xlsx\n",
      "✅ Processed: Embu_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Embu_gendered_enterprise.xlsx\n",
      "✅ Processed: Garissa_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Garissa_gendered_enterprise.xlsx\n",
      "✅ Processed: Homa_Bay_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Homa_gendered_enterprise.xlsx\n",
      "✅ Processed: Isiolo_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Isiolo_gendered_enterprise.xlsx\n",
      "✅ Processed: Kajiado_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kajiado_gendered_enterprise.xlsx\n",
      "✅ Processed: Kakamega_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kakamega_gendered_enterprise.xlsx\n",
      "✅ Processed: Kericho_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kericho_gendered_enterprise.xlsx\n",
      "✅ Processed: Kiambu_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kiambu_gendered_enterprise.xlsx\n",
      "✅ Processed: Kilifi_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kilifi_gendered_enterprise.xlsx\n",
      "✅ Processed: Kirinyaga_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kirinyaga_gendered_enterprise.xlsx\n",
      "✅ Processed: Kisii_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kisii_gendered_enterprise.xlsx\n",
      "✅ Processed: Kisumu_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kisumu_gendered_enterprise.xlsx\n",
      "✅ Processed: Kitui_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kitui_gendered_enterprise.xlsx\n",
      "✅ Processed: Kwale_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kwale_gendered_enterprise.xlsx\n",
      "✅ Processed: Laikipia_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Laikipia_gendered_enterprise.xlsx\n",
      "✅ Processed: Machakos_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Machakos_gendered_enterprise.xlsx\n",
      "✅ Processed: Migori_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Migori_gendered_enterprise.xlsx\n",
      "✅ Processed: Murang’a_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Murang’a_gendered_enterprise.xlsx\n",
      "✅ Processed: Nakuru_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Nakuru_gendered_enterprise.xlsx\n",
      "✅ Processed: Nandi_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Nandi_gendered_enterprise.xlsx\n",
      "✅ Processed: Narok_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Narok_gendered_enterprise.xlsx\n",
      "✅ Processed: Nyamira_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Nyamira_gendered_enterprise.xlsx\n",
      "✅ Processed: Nyandarua_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Nyandarua_gendered_enterprise.xlsx\n",
      "✅ Processed: Nyeri_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Nyeri_gendered_enterprise.xlsx\n",
      "✅ Processed: Siaya_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Siaya_gendered_enterprise.xlsx\n",
      "✅ Processed: Taita_Taveta_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Taita_gendered_enterprise.xlsx\n",
      "✅ Processed: Tana_River_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Tana_gendered_enterprise.xlsx\n",
      "✅ Processed: Tharaka_Nithi_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Tharaka_gendered_enterprise.xlsx\n",
      "✅ Processed: Trans-Nzoia_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Trans-Nzoia_gendered_enterprise.xlsx\n",
      "✅ Processed: Turkana_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Turkana_gendered_enterprise.xlsx\n",
      "✅ Processed: Uasin_Gishu_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Uasin_gendered_enterprise.xlsx\n",
      "✅ Processed: Vihiga_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Vihiga_gendered_enterprise.xlsx\n",
      "✅ Processed: West_Pokot_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\West_gendered_enterprise.xlsx\n",
      "🎯 Enhanced extraction completed with full table handling.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# ==========================\n",
    "# 1️⃣ AUTO-DETECT WORKING DIRECTORY\n",
    "# ==========================\n",
    "try:\n",
    "    BASE_DIR = os.path.dirname(os.path.abspath(__file__))  # If running as script\n",
    "except NameError:\n",
    "    BASE_DIR = os.getcwd()  # If running interactively\n",
    "\n",
    "INPUT_DIR = os.path.join(BASE_DIR, \"Questionnaires\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"extracted_excel\")\n",
    "LOOKUP_JSON = os.path.join(BASE_DIR, \"lookup.json\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ==========================\n",
    "# 2️⃣ LOAD MASTER QUESTION MAPPING\n",
    "# ==========================\n",
    "with open(LOOKUP_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    QUESTION_MAPPING = json.load(f)\n",
    "\n",
    "# ==========================\n",
    "# 3️⃣ CLEAN TEXT FUNCTION\n",
    "# ==========================\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove unwanted characters and normalize spacing.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = str(text).strip()\n",
    "    text = text.replace(\"✔\", \" Selected \").replace(\"✓\", \" Selected \")\n",
    "    text = re.sub(r'[\\u2022•■▪]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# ==========================\n",
    "# 4️⃣ CONVERT TABLE ROW TO SENTENCE\n",
    "# ==========================\n",
    "def row_to_sentence(cells):\n",
    "    \"\"\"Converts a row (list of cell strings) into a readable sentence.\"\"\"\n",
    "    if len(cells) == 2:\n",
    "        return f\"{cells[0]} has {cells[1]}.\"\n",
    "    return \" \".join(cells)\n",
    "\n",
    "# ==========================\n",
    "# 5️⃣ PROCESS DOCX FILE CONTENT\n",
    "# ==========================\n",
    "def process_docx(filepath):\n",
    "    \"\"\"\n",
    "    Extracts paragraphs and all tables from a Word docx file,\n",
    "    converting every table row into a readable sentence.\n",
    "    \"\"\"\n",
    "    doc = Document(filepath)\n",
    "    text_data = []\n",
    "\n",
    "    # --- Extract paragraphs ---\n",
    "    for para in doc.paragraphs:\n",
    "        txt = clean_text(para.text)\n",
    "        if txt:\n",
    "            text_data.append(txt)\n",
    "\n",
    "    # --- Extract every table row ---\n",
    "    for table in doc.tables:\n",
    "        for row in table.rows:\n",
    "            cells = [clean_text(cell.text) for cell in row.cells if clean_text(cell.text)]\n",
    "            if cells:\n",
    "                if len(cells) == 2:\n",
    "                    sentence = f\"{cells[0]} has {cells[1]}.\"\n",
    "                else:\n",
    "                    sentence = \" \".join(cells)\n",
    "                text_data.append(sentence)\n",
    "\n",
    "    return text_data\n",
    "\n",
    "# ==========================\n",
    "# 6️⃣ FIND BEST MATCHING RESPONSE\n",
    "# ==========================\n",
    "def find_response(all_text, variants, threshold=70):\n",
    "    \"\"\"Finds the best matching question and extracts its answer.\"\"\"\n",
    "    best_score = 0\n",
    "    best_index = -1\n",
    "\n",
    "    for variant in variants:\n",
    "        for i, line in enumerate(all_text):\n",
    "            score = fuzz.partial_ratio(variant.lower(), line.lower())\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_index = i\n",
    "\n",
    "    if best_score < threshold or best_index == -1:\n",
    "        return \"\"\n",
    "\n",
    "    response_lines = []\n",
    "    for j in range(best_index + 1, len(all_text)):\n",
    "        line = all_text[j].strip()\n",
    "        if not line:\n",
    "            break\n",
    "        if line.endswith(\"?\"):\n",
    "            break\n",
    "        if any(fuzz.partial_ratio(line.lower(), v.lower()) >= 65 for v in variants):\n",
    "            break\n",
    "        if re.match(r\"^(how|what|when|why|which|who)\\b\", line.lower()):\n",
    "            break\n",
    "        response_lines.append(line)\n",
    "\n",
    "    response = \" \".join(response_lines).strip()\n",
    "    if not response or response.lower().startswith((\"how\", \"what\", \"when\", \"which\", \"who\")):\n",
    "        return \"\"\n",
    "\n",
    "    return response\n",
    "\n",
    "# ==========================\n",
    "# 7️⃣ MAIN PROCESSING LOOP\n",
    "# ==========================\n",
    "for file in os.listdir(INPUT_DIR):\n",
    "    if not file.endswith(\".docx\"):\n",
    "        continue\n",
    "\n",
    "    county = file.split(\"_\")[0].strip()\n",
    "    filepath = os.path.join(INPUT_DIR, file)\n",
    "\n",
    "    text_data = process_docx(filepath)\n",
    "\n",
    "    output_rows = []\n",
    "    for theme, questions in QUESTION_MAPPING.items():\n",
    "        for master_question, variants in questions.items():\n",
    "            response = find_response(text_data, variants)\n",
    "            output_rows.append({\n",
    "                \"County\": county,\n",
    "                \"Theme\": theme,\n",
    "                \"Question\": master_question,\n",
    "                \"Response\": response\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(output_rows)\n",
    "    out_file = os.path.join(OUTPUT_DIR, f\"{county}_gendered_enterprise.xlsx\")\n",
    "    df.to_excel(out_file, index=False)\n",
    "\n",
    "    print(f\"✅ Processed: {file} -> {out_file}\")\n",
    "\n",
    "print(\"🎯 Enhanced extraction completed with full table handling.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "275a3cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed: Baringo_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Baringo_gendered_enterprise.xlsx\n",
      "✅ Processed: Bomet_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Bomet_gendered_enterprise.xlsx\n",
      "✅ Processed: Bungoma_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Bungoma_gendered_enterprise.xlsx\n",
      "✅ Processed: Busia_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Busia_gendered_enterprise.xlsx\n",
      "✅ Processed: Elgeyo_Marakwet_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Elgeyo_gendered_enterprise.xlsx\n",
      "✅ Processed: Embu_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Embu_gendered_enterprise.xlsx\n",
      "✅ Processed: Garissa_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Garissa_gendered_enterprise.xlsx\n",
      "✅ Processed: Homa_Bay_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Homa_gendered_enterprise.xlsx\n",
      "✅ Processed: Isiolo_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Isiolo_gendered_enterprise.xlsx\n",
      "✅ Processed: Kajiado_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kajiado_gendered_enterprise.xlsx\n",
      "✅ Processed: Kakamega_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kakamega_gendered_enterprise.xlsx\n",
      "✅ Processed: Kericho_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kericho_gendered_enterprise.xlsx\n",
      "✅ Processed: Kiambu_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kiambu_gendered_enterprise.xlsx\n",
      "✅ Processed: Kilifi_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kilifi_gendered_enterprise.xlsx\n",
      "✅ Processed: Kirinyaga_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kirinyaga_gendered_enterprise.xlsx\n",
      "✅ Processed: Kisii_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kisii_gendered_enterprise.xlsx\n",
      "✅ Processed: Kisumu_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kisumu_gendered_enterprise.xlsx\n",
      "✅ Processed: Kitui_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kitui_gendered_enterprise.xlsx\n",
      "✅ Processed: Kwale_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Kwale_gendered_enterprise.xlsx\n",
      "✅ Processed: Laikipia_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Laikipia_gendered_enterprise.xlsx\n",
      "✅ Processed: Machakos_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Machakos_gendered_enterprise.xlsx\n",
      "✅ Processed: Migori_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Migori_gendered_enterprise.xlsx\n",
      "✅ Processed: Murang’a_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Murang’a_gendered_enterprise.xlsx\n",
      "✅ Processed: Nakuru_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Nakuru_gendered_enterprise.xlsx\n",
      "✅ Processed: Nandi_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Nandi_gendered_enterprise.xlsx\n",
      "✅ Processed: Narok_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Narok_gendered_enterprise.xlsx\n",
      "✅ Processed: Nyamira_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Nyamira_gendered_enterprise.xlsx\n",
      "✅ Processed: Nyandarua_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Nyandarua_gendered_enterprise.xlsx\n",
      "✅ Processed: Nyeri_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Nyeri_gendered_enterprise.xlsx\n",
      "✅ Processed: Siaya_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Siaya_gendered_enterprise.xlsx\n",
      "✅ Processed: Taita_Taveta_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Taita_gendered_enterprise.xlsx\n",
      "✅ Processed: Tana_River_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Tana_gendered_enterprise.xlsx\n",
      "✅ Processed: Tharaka_Nithi_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Tharaka_gendered_enterprise.xlsx\n",
      "✅ Processed: Trans-Nzoia_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Trans-Nzoia_gendered_enterprise.xlsx\n",
      "✅ Processed: Turkana_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Turkana_gendered_enterprise.xlsx\n",
      "✅ Processed: Uasin_Gishu_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Uasin_gendered_enterprise.xlsx\n",
      "✅ Processed: Vihiga_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\Vihiga_gendered_enterprise.xlsx\n",
      "✅ Processed: West_Pokot_gendered_enterprise_selection_interview_guide.docx -> d:\\AAAA_Data\\GENDER\\extracted_excel\\West_gendered_enterprise.xlsx\n",
      "🎯 Extraction completed with full table support (docx2python).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from docx2python import docx2python\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# ==========================\n",
    "# 1. AUTO-DETECT WORKING DIRECTORY\n",
    "# ==========================\n",
    "try:\n",
    "    BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    BASE_DIR = os.getcwd()\n",
    "\n",
    "INPUT_DIR = os.path.join(BASE_DIR, \"Questionnaires\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"extracted_excel\")\n",
    "LOOKUP_JSON = os.path.join(BASE_DIR, \"lookup.json\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ==========================\n",
    "# 2. LOAD MASTER QUESTION MAPPING\n",
    "# ==========================\n",
    "with open(LOOKUP_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    QUESTION_MAPPING = json.load(f)\n",
    "\n",
    "# ==========================\n",
    "# 3. CLEAN TEXT\n",
    "# ==========================\n",
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = str(text).replace(\"\\n\", \" \").strip()\n",
    "    text = text.replace(\"✔\", \" Selected \").replace(\"✓\", \" Selected \")\n",
    "    text = re.sub(r'[\\u2022•■▪]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# ==========================\n",
    "# 4. PROCESS DOCX FILE WITH TABLES\n",
    "# ==========================\n",
    "def process_docx(filepath):\n",
    "    \"\"\"Extracts all text including tables from DOCX using docx2python.\"\"\"\n",
    "    doc = docx2python(filepath, html=False)\n",
    "    text_data = []\n",
    "\n",
    "    # Extract paragraphs\n",
    "    for page in doc.text.split(\"\\n\"):\n",
    "        txt = clean_text(page)\n",
    "        if txt:\n",
    "            text_data.append(txt)\n",
    "\n",
    "    # Extract tables explicitly\n",
    "    for tbl in doc.body:  # doc.body → pages → tables\n",
    "        for row in tbl:\n",
    "            if isinstance(row, list):\n",
    "                row_cells = [clean_text(\" \".join(cell)) if isinstance(cell, list) else clean_text(cell) for cell in row]\n",
    "                row_line = \" | \".join([c for c in row_cells if c])\n",
    "                if row_line:\n",
    "                    text_data.append(row_line)\n",
    "\n",
    "    return text_data\n",
    "\n",
    "# ==========================\n",
    "# 5. FIND RESPONSE\n",
    "# ==========================\n",
    "def find_response(all_text, variants, threshold=70):\n",
    "    best_score = 0\n",
    "    best_index = -1\n",
    "\n",
    "    for variant in variants:\n",
    "        for i, line in enumerate(all_text):\n",
    "            score = fuzz.partial_ratio(variant.lower(), line.lower())\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_index = i\n",
    "\n",
    "    if best_score < threshold or best_index == -1:\n",
    "        return \"\"\n",
    "\n",
    "    response_lines = []\n",
    "    for j in range(best_index + 1, len(all_text)):\n",
    "        line = all_text[j].strip()\n",
    "        if not line:\n",
    "            break\n",
    "        if line.endswith(\"?\") or re.match(r\"^(how|what|when|which|who)\\b\", line.lower()):\n",
    "            break\n",
    "        if any(fuzz.partial_ratio(line.lower(), v.lower()) >= 65 for v in variants):\n",
    "            break\n",
    "        response_lines.append(line)\n",
    "\n",
    "    return \" \".join(response_lines).strip()\n",
    "\n",
    "# ==========================\n",
    "# 6. MAIN LOOP\n",
    "# ==========================\n",
    "for file in os.listdir(INPUT_DIR):\n",
    "    if not file.endswith(\".docx\"):\n",
    "        continue\n",
    "\n",
    "    county = file.split(\"_\")[0].strip()\n",
    "    filepath = os.path.join(INPUT_DIR, file)\n",
    "\n",
    "    text_data = process_docx(filepath)\n",
    "\n",
    "    output_rows = []\n",
    "    for theme, questions in QUESTION_MAPPING.items():\n",
    "        for master_question, variants in questions.items():\n",
    "            response = find_response(text_data, variants)\n",
    "            output_rows.append({\n",
    "                \"County\": county,\n",
    "                \"Theme\": theme,\n",
    "                \"Question\": master_question,\n",
    "                \"Response\": response\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(output_rows)\n",
    "    out_file = os.path.join(OUTPUT_DIR, f\"{county}_gendered_enterprise.xlsx\")\n",
    "    df.to_excel(out_file, index=False)\n",
    "\n",
    "    print(f\"✅ Processed: {file} -> {out_file}\")\n",
    "\n",
    "print(\"🎯 Extraction completed with full table support (docx2python).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4934ca9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Processing Baringo_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Baringo_gendered_enterprise.xlsx\n",
      "🔹 Processing Bomet_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Bomet_gendered_enterprise.xlsx\n",
      "🔹 Processing Bungoma_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Bungoma_gendered_enterprise.xlsx\n",
      "🔹 Processing Busia_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Busia_gendered_enterprise.xlsx\n",
      "🔹 Processing Elgeyo_Marakwet_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Elgeyo_gendered_enterprise.xlsx\n",
      "🔹 Processing Embu_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Embu_gendered_enterprise.xlsx\n",
      "🔹 Processing Garissa_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Garissa_gendered_enterprise.xlsx\n",
      "🔹 Processing Homa_Bay_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Homa_gendered_enterprise.xlsx\n",
      "🔹 Processing Isiolo_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Isiolo_gendered_enterprise.xlsx\n",
      "🔹 Processing Kajiado_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Kajiado_gendered_enterprise.xlsx\n",
      "🔹 Processing Kakamega_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Kakamega_gendered_enterprise.xlsx\n",
      "🔹 Processing Kericho_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Kericho_gendered_enterprise.xlsx\n",
      "🔹 Processing Kiambu_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Kiambu_gendered_enterprise.xlsx\n",
      "🔹 Processing Kilifi_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Kilifi_gendered_enterprise.xlsx\n",
      "🔹 Processing Kirinyaga_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Kirinyaga_gendered_enterprise.xlsx\n",
      "🔹 Processing Kisii_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Kisii_gendered_enterprise.xlsx\n",
      "🔹 Processing Kisumu_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Kisumu_gendered_enterprise.xlsx\n",
      "🔹 Processing Kitui_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Kitui_gendered_enterprise.xlsx\n",
      "🔹 Processing Kwale_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Kwale_gendered_enterprise.xlsx\n",
      "🔹 Processing Laikipia_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Laikipia_gendered_enterprise.xlsx\n",
      "🔹 Processing Machakos_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Machakos_gendered_enterprise.xlsx\n",
      "🔹 Processing Migori_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Migori_gendered_enterprise.xlsx\n",
      "🔹 Processing Murang’a_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Murang’a_gendered_enterprise.xlsx\n",
      "🔹 Processing Nakuru_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Nakuru_gendered_enterprise.xlsx\n",
      "🔹 Processing Nandi_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Nandi_gendered_enterprise.xlsx\n",
      "🔹 Processing Narok_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Narok_gendered_enterprise.xlsx\n",
      "🔹 Processing Nyamira_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Nyamira_gendered_enterprise.xlsx\n",
      "🔹 Processing Nyandarua_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Nyandarua_gendered_enterprise.xlsx\n",
      "🔹 Processing Nyeri_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Nyeri_gendered_enterprise.xlsx\n",
      "🔹 Processing Siaya_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Siaya_gendered_enterprise.xlsx\n",
      "🔹 Processing Taita_Taveta_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Taita_gendered_enterprise.xlsx\n",
      "🔹 Processing Tana_River_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Tana_gendered_enterprise.xlsx\n",
      "🔹 Processing Tharaka_Nithi_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Tharaka_gendered_enterprise.xlsx\n",
      "🔹 Processing Trans-Nzoia_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Trans-Nzoia_gendered_enterprise.xlsx\n",
      "🔹 Processing Turkana_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Turkana_gendered_enterprise.xlsx\n",
      "🔹 Processing Uasin_Gishu_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Uasin_gendered_enterprise.xlsx\n",
      "🔹 Processing Vihiga_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\Vihiga_gendered_enterprise.xlsx\n",
      "🔹 Processing West_Pokot_gendered_enterprise_selection_interview_guide.docx ...\n",
      "✅ Saved: d:\\AAAA_Data\\GENDER\\extracted_excel\\West_gendered_enterprise.xlsx\n",
      "🎯 Extraction completed with improved table support (docx2python + merge spillover tables).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from docx2python import docx2python\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# ==========================\n",
    "# 1️⃣ AUTO-DETECT WORKING DIRECTORY\n",
    "# ==========================\n",
    "try:\n",
    "    BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    BASE_DIR = os.getcwd()\n",
    "\n",
    "INPUT_DIR = os.path.join(BASE_DIR, \"Questionnaires\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"extracted_excel\")\n",
    "LOOKUP_JSON = os.path.join(BASE_DIR, \"lookup.json\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ==========================\n",
    "# 2️⃣ LOAD MASTER QUESTION MAPPING\n",
    "# ==========================\n",
    "with open(LOOKUP_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    QUESTION_MAPPING = json.load(f)\n",
    "\n",
    "# ==========================\n",
    "# 3️⃣ CLEAN TEXT\n",
    "# ==========================\n",
    "def clean_text(text):\n",
    "    \"\"\"Normalize whitespace and remove special characters.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = str(text).replace(\"\\n\", \" \").replace(\"✔\", \" Selected \").replace(\"✓\", \" Selected \")\n",
    "    text = re.sub(r'[\\u2022•■▪]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# ==========================\n",
    "# 4️⃣ MERGE SPLIT TABLES ACROSS PAGES\n",
    "# ==========================\n",
    "def merge_split_tables(tables):\n",
    "    \"\"\"\n",
    "    Merge tables that were split across pages.\n",
    "    Tables with similar column counts or missing headers\n",
    "    are assumed to be continuations of the previous table.\n",
    "    \"\"\"\n",
    "    merged = []\n",
    "    for tbl in tables:\n",
    "        rows = []\n",
    "        for row in tbl:\n",
    "            if isinstance(row, list):\n",
    "                row_txt = [clean_text(\" \".join(cell)) if isinstance(cell, list) else clean_text(cell) for cell in row]\n",
    "                row_txt = [c for c in row_txt if c]\n",
    "                if row_txt:\n",
    "                    rows.append(row_txt)\n",
    "        if not rows:\n",
    "            continue\n",
    "\n",
    "        if merged and len(rows[0]) <= len(merged[-1][0]):\n",
    "            # Append continuation\n",
    "            merged[-1].extend(rows)\n",
    "        else:\n",
    "            merged.append(rows)\n",
    "    return merged\n",
    "\n",
    "# ==========================\n",
    "# 5️⃣ PROCESS DOCX FILE\n",
    "# ==========================\n",
    "def process_docx(filepath):\n",
    "    \"\"\"Extracts paragraphs and table rows (merged) from DOCX.\"\"\"\n",
    "    doc = docx2python(filepath, html=False)\n",
    "    text_data = []\n",
    "\n",
    "    # --- Extract paragraphs ---\n",
    "    for page in doc.text.split(\"\\n\"):\n",
    "        txt = clean_text(page)\n",
    "        if txt:\n",
    "            text_data.append(txt)\n",
    "\n",
    "    # --- Extract tables and merge spillovers ---\n",
    "    all_tables = []\n",
    "    for tbl in doc.body:\n",
    "        if isinstance(tbl, list):\n",
    "            all_tables.append(tbl)\n",
    "\n",
    "    merged_tables = merge_split_tables(all_tables)\n",
    "\n",
    "    for tbl in merged_tables:\n",
    "        for row in tbl:\n",
    "            if row:\n",
    "                text_data.append(\" | \".join(row))\n",
    "\n",
    "    return text_data\n",
    "\n",
    "# ==========================\n",
    "# 6️⃣ FIND BEST MATCHING RESPONSE\n",
    "# ==========================\n",
    "def find_response(all_text, variants, threshold=70):\n",
    "    \"\"\"Finds the response following the best matching question.\"\"\"\n",
    "    best_score = 0\n",
    "    best_index = -1\n",
    "\n",
    "    for variant in variants:\n",
    "        for i, line in enumerate(all_text):\n",
    "            score = fuzz.partial_ratio(variant.lower(), line.lower())\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_index = i\n",
    "\n",
    "    if best_score < threshold or best_index == -1:\n",
    "        return \"\"\n",
    "\n",
    "    response_lines = []\n",
    "    for j in range(best_index + 1, len(all_text)):\n",
    "        line = all_text[j].strip()\n",
    "        if not line:\n",
    "            break\n",
    "        # Stop if line looks like next question\n",
    "        if line.endswith(\"?\") or re.match(r\"^(how|what|when|which|who)\\b\", line.lower()):\n",
    "            break\n",
    "        if any(fuzz.partial_ratio(line.lower(), v.lower()) >= 65 for v in variants):\n",
    "            break\n",
    "        response_lines.append(line)\n",
    "\n",
    "    return \" \".join(response_lines).strip()\n",
    "\n",
    "# ==========================\n",
    "# 7️⃣ MAIN PROCESSING LOOP\n",
    "# ==========================\n",
    "for file in os.listdir(INPUT_DIR):\n",
    "    if not file.endswith(\".docx\"):\n",
    "        continue\n",
    "\n",
    "    county = file.split(\"_\")[0].strip()\n",
    "    filepath = os.path.join(INPUT_DIR, file)\n",
    "\n",
    "    print(f\"🔹 Processing {file} ...\")\n",
    "    text_data = process_docx(filepath)\n",
    "\n",
    "    output_rows = []\n",
    "    for theme, questions in QUESTION_MAPPING.items():\n",
    "        for master_question, variants in questions.items():\n",
    "            response = find_response(text_data, variants)\n",
    "            output_rows.append({\n",
    "                \"County\": county,\n",
    "                \"Theme\": theme,\n",
    "                \"Question\": master_question,\n",
    "                \"Response\": response\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(output_rows)\n",
    "    out_file = os.path.join(OUTPUT_DIR, f\"{county}_gendered_enterprise.xlsx\")\n",
    "    df.to_excel(out_file, index=False)\n",
    "\n",
    "    print(f\"✅ Saved: {out_file}\")\n",
    "\n",
    "print(\"🎯 Extraction completed with improved table support (docx2python + merge spillover tables).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
